{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_flair</th>\n",
       "      <th>original_shape_Elongation_flair</th>\n",
       "      <th>original_shape_Flatness_flair</th>\n",
       "      <th>original_shape_LeastAxisLength_flair</th>\n",
       "      <th>original_shape_MajorAxisLength_flair</th>\n",
       "      <th>original_shape_Maximum2DDiameterColumn_flair</th>\n",
       "      <th>original_shape_Maximum2DDiameterRow_flair</th>\n",
       "      <th>original_shape_Maximum2DDiameterSlice_flair</th>\n",
       "      <th>original_shape_Maximum3DDiameter_flair</th>\n",
       "      <th>original_shape_MeshVolume_flair</th>\n",
       "      <th>...</th>\n",
       "      <th>exponential_glszm_SmallAreaHighGrayLevelEmphasis_t2</th>\n",
       "      <th>exponential_glszm_SmallAreaLowGrayLevelEmphasis_t2</th>\n",
       "      <th>exponential_glszm_ZoneEntropy_t2</th>\n",
       "      <th>exponential_glszm_ZonePercentage_t2</th>\n",
       "      <th>exponential_glszm_ZoneVariance_t2</th>\n",
       "      <th>exponential_ngtdm_Busyness_t2</th>\n",
       "      <th>exponential_ngtdm_Coarseness_t2</th>\n",
       "      <th>exponential_ngtdm_Complexity_t2</th>\n",
       "      <th>exponential_ngtdm_Contrast_t2</th>\n",
       "      <th>exponential_ngtdm_Strength_t2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.963943</td>\n",
       "      <td>0.705955</td>\n",
       "      <td>43.311174</td>\n",
       "      <td>61.351145</td>\n",
       "      <td>72.449983</td>\n",
       "      <td>95.015788</td>\n",
       "      <td>85.586214</td>\n",
       "      <td>95.963535</td>\n",
       "      <td>52824.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.922060</td>\n",
       "      <td>0.254117</td>\n",
       "      <td>4.887784</td>\n",
       "      <td>0.002771</td>\n",
       "      <td>7.763001e+06</td>\n",
       "      <td>134.634253</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>16.490983</td>\n",
       "      <td>0.001646</td>\n",
       "      <td>0.077194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.832035</td>\n",
       "      <td>0.509359</td>\n",
       "      <td>40.699397</td>\n",
       "      <td>79.903116</td>\n",
       "      <td>78.924014</td>\n",
       "      <td>79.378838</td>\n",
       "      <td>77.794601</td>\n",
       "      <td>83.318665</td>\n",
       "      <td>73177.83333</td>\n",
       "      <td>...</td>\n",
       "      <td>5.065441</td>\n",
       "      <td>0.209623</td>\n",
       "      <td>5.045165</td>\n",
       "      <td>0.001858</td>\n",
       "      <td>1.102532e+07</td>\n",
       "      <td>241.479082</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>9.259663</td>\n",
       "      <td>0.009020</td>\n",
       "      <td>0.010283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 8616 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label_flair  original_shape_Elongation_flair  \\\n",
       "0            1                         0.963943   \n",
       "1            1                         0.832035   \n",
       "\n",
       "   original_shape_Flatness_flair  original_shape_LeastAxisLength_flair  \\\n",
       "0                       0.705955                             43.311174   \n",
       "1                       0.509359                             40.699397   \n",
       "\n",
       "   original_shape_MajorAxisLength_flair  \\\n",
       "0                             61.351145   \n",
       "1                             79.903116   \n",
       "\n",
       "   original_shape_Maximum2DDiameterColumn_flair  \\\n",
       "0                                     72.449983   \n",
       "1                                     78.924014   \n",
       "\n",
       "   original_shape_Maximum2DDiameterRow_flair  \\\n",
       "0                                  95.015788   \n",
       "1                                  79.378838   \n",
       "\n",
       "   original_shape_Maximum2DDiameterSlice_flair  \\\n",
       "0                                    85.586214   \n",
       "1                                    77.794601   \n",
       "\n",
       "   original_shape_Maximum3DDiameter_flair  original_shape_MeshVolume_flair  \\\n",
       "0                               95.963535                      52824.00000   \n",
       "1                               83.318665                      73177.83333   \n",
       "\n",
       "   ...  exponential_glszm_SmallAreaHighGrayLevelEmphasis_t2  \\\n",
       "0  ...                                           2.922060     \n",
       "1  ...                                           5.065441     \n",
       "\n",
       "   exponential_glszm_SmallAreaLowGrayLevelEmphasis_t2  \\\n",
       "0                                           0.254117    \n",
       "1                                           0.209623    \n",
       "\n",
       "   exponential_glszm_ZoneEntropy_t2  exponential_glszm_ZonePercentage_t2  \\\n",
       "0                          4.887784                             0.002771   \n",
       "1                          5.045165                             0.001858   \n",
       "\n",
       "   exponential_glszm_ZoneVariance_t2  exponential_ngtdm_Busyness_t2  \\\n",
       "0                       7.763001e+06                     134.634253   \n",
       "1                       1.102532e+07                     241.479082   \n",
       "\n",
       "   exponential_ngtdm_Coarseness_t2  exponential_ngtdm_Complexity_t2  \\\n",
       "0                         0.000290                        16.490983   \n",
       "1                         0.000204                         9.259663   \n",
       "\n",
       "   exponential_ngtdm_Contrast_t2  exponential_ngtdm_Strength_t2  \n",
       "0                       0.001646                       0.077194  \n",
       "1                       0.009020                       0.010283  \n",
       "\n",
       "[2 rows x 8616 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import numpy as np\n",
    "#pd.set_option('display.max_columns', None)\n",
    "#T1_Data = pd.read_csv('E:\\\\Data_cjh\\\\BraTS2019\\\\test\\\\t1_radiomics_features.csv')E:\\Data_cjh\\BraTS2019\\radimoic_feature\\BraTS_2017\n",
    "\n",
    "#'E:\\\\Data_cjh\\\\BraTS2019\\\\Feature_brats_2019\\\\WTandET\\\\WT\\\\WT_t1_features.csv'\n",
    "#T1_Data = pd.read_csv('G:\\\\MediclImage\\\\Data\\\\NeckFibrosis\\\\new\\\\T1_C_PreRT_ok.csv')\n",
    "Flair_Data = pd.read_csv('H:/Data_cjh/BraTS2019/radimoic_feature/peritumoral/Dataset/Dataset/17-ED-finished/flair_selected_Features.csv')\n",
    "\n",
    "T1_Data = pd.read_csv('H:/Data_cjh/BraTS2019/radimoic_feature/peritumoral/Dataset/Dataset/17-ED-finished/t1_selected_Features.csv')\n",
    "\n",
    "T1ce_Data = pd.read_csv('H:/Data_cjh/BraTS2019/radimoic_feature/peritumoral/Dataset/Dataset/17-ED-finished/t1ce_selected_Features.csv')\n",
    "\n",
    "T2_Data = pd.read_csv('H:/Data_cjh/BraTS2019/radimoic_feature/peritumoral//Dataset/Dataset/17-ED-finished/t2_selected_Features.csv')\n",
    "\n",
    "Flair_Data = shuffle(Flair_Data,random_state = 1234)\n",
    "T1_Data = shuffle(T1_Data,random_state = 1234)\n",
    "T1ce_Data = shuffle(T1ce_Data,random_state = 1234)\n",
    "T2_Data = shuffle(T2_Data,random_state = 1234)\n",
    "\n",
    "Flair_Data.columns = [\"{}_flair\".format(Flair_Data.columns[i]) for i in range(len(Flair_Data.columns))]\n",
    "T1_Data.columns = [\"{}_t1\".format(T1_Data.columns[i]) for i in range(len(T1_Data.columns))]\n",
    "T1ce_Data.columns = [\"{}_t1ce\".format(T1ce_Data.columns[i]) for i in range(len(T1ce_Data.columns))]\n",
    "T2_Data.columns = [\"{}_t2\".format(T2_Data.columns[i]) for i in range(len(T2_Data.columns))]\n",
    "\n",
    "Multi_Data = pd.concat([Flair_Data,T1_Data,T1ce_Data,T2_Data],axis=1,verify_integrity=False)\n",
    "Multi_Data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Flair = Flair_Data.drop(['label_flair'], axis=1)\n",
    "Y_Flair = Flair_Data.loc[:,['label_flair']]\n",
    "X_T1ce = T1ce_Data.drop(['label_t1ce'], axis=1)\n",
    "Y_T1ce = T1ce_Data.loc[:,['label_t1ce']]\n",
    "                         \n",
    "X_T2 = T2_Data.drop(['label_t2'], axis=1)\n",
    "Y_T2 = T2_Data.loc[:,['label_t2']]\n",
    "X_T1 = T1_Data.drop(['label_t1'], axis=1)\n",
    "Y_T1 = T1_Data.loc[:,['label_t1']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((284, 2153),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       dtype=int64))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler =StandardScaler()\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_Flair_data = scaler.fit_transform(X_Flair)\n",
    "X_T1ce_data = scaler.fit_transform(X_T1ce)\n",
    "X_T2_data = scaler.fit_transform(X_T2)\n",
    "X_T1_data = scaler.fit_transform(X_T1)\n",
    "\n",
    "class_labels = LabelEncoder()\n",
    "\n",
    "Y_data = class_labels.fit_transform(Y_Flair.values.ravel())\n",
    "\n",
    "X_Flair_data.shape,Y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold \n",
    "def variance_threshold_selector(data, threshold=0.009):\n",
    "    selector = VarianceThreshold(threshold)\n",
    "    selector.fit(data)\n",
    "    return data[data.columns[selector.get_support(indices=True)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV, LassoLars, LassoLarsIC,Lasso,ARDRegression,RidgeCV,ElasticNetCV\n",
    "\n",
    "def feacture_selection(X_train,Y_train):\n",
    "    #regr = ElasticNetCV(l1_ratio=1,max_iter=10000, n_alphas=100,cv=10,normalize =True, random_state=1003,n_jobs=4,tol=0.00001).fit(X_train, Y_train)\n",
    "    regr = ElasticNetCV(l1_ratio=1,cv=5,normalize =True, random_state=1003,n_jobs=4).fit(X_train, Y_train)\n",
    "    #regr = LassoCV(cv=5, random_state=0).fit(X_train, Y_train)\n",
    "    reg = regr.coef_\n",
    "    columns = X_train.columns\n",
    "    feature_columns = []\n",
    "    feature_corff = []\n",
    "    indexs = []\n",
    "    for index in range(X_train.columns.shape[0]):\n",
    "        if reg[index] != 0.0:\n",
    "            feature_columns.append(columns[index])\n",
    "            feature_corff.append(reg[index])\n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mifs\n",
    "def mRMR_feature_select(X_train,Y_train,mRMR_feature_num=30):\n",
    "   \n",
    "    print(\"mRMR feature start...\")\n",
    "    feat_selector = mifs.MutualInformationFeatureSelector('MRMR', n_features=mRMR_feature_num,n_jobs=3, verbose=0)\n",
    "    feat_selector.fit(X_train, Y_train)\n",
    "    print(\"mRMR feature end...\")\n",
    "    feature_mifs = X_train.columns[feat_selector.ranking_].tolist()\n",
    "    print(\" the mRMR select number of features is \",len(feature_mifs), feature_mifs)\n",
    "    return feature_mifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report,roc_auc_score,f1_score\n",
    "def evalution_metirc(y_test,y_pred_1,y_pred,labels,target_names):\n",
    "    \n",
    "    auc = roc_auc_score(y_test, y_pred_1)\n",
    "    print('ROC AUC: %f' % auc)\n",
    "#     模型评估之混淆矩阵\n",
    "    confusion = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    if float(np.sum(confusion)) != 0:\n",
    "        accuracy = float(confusion[0, 0] + confusion[1, 1]) / float(np.sum(confusion))\n",
    "    #print(\"Global Accuracy: \" + str(accuracy))\n",
    "    specificity = 0\n",
    "    if float(confusion[0, 0] + confusion[0, 1]) != 0:\n",
    "        specificity = float(confusion[0, 0]) / float(confusion[0, 0] + confusion[0, 1])\n",
    "    #print(\"Specificity: \" + str(specificity))\n",
    "    sensitivity = 0\n",
    "    if float(confusion[1, 1] + confusion[1, 0]) != 0:\n",
    "        sensitivity = float(confusion[1, 1]) / float(confusion[1, 1] + confusion[1, 0])\n",
    "    #print(\"Sensitivity: \" + str(sensitivity))\n",
    "    # make predictions for test data and evaluate\n",
    "\n",
    "    print(confusion)\n",
    "    print(classification_report(y_test, y_pred, labels=labels, target_names=target_names))\n",
    "    F1_score = f1_score(y_test, y_pred, labels=labels, pos_label=1)\n",
    "    return auc,accuracy,specificity,sensitivity,F1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "def smote_augment_data(X_train,y_train):\n",
    "    x_columns = X_train.columns\n",
    "    y_columns = y_train.columns\n",
    "    #注意过采样时 只对训练集进行过采样\n",
    "    oversampler=SMOTE(random_state=0)\n",
    "    x_smote_train,y_smote_train=oversampler.fit_sample(X_train,y_train)\n",
    "    x_smote_train = pd.DataFrame(x_smote_train, columns=x_columns)\n",
    "    y_smote_train = pd.DataFrame(y_smote_train, columns=y_columns)\n",
    "    return x_smote_train,y_smote_train\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "def grid_search_cv(model,param_grid,X,Y):\n",
    "    grid_search = GridSearchCV(model,param_grid,n_jobs=2,verbose=1,cv=5)\n",
    "    grid_search.fit(X,Y)\n",
    "    return grid_search.best_estimator_.get_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interp\n",
    "from sklearn.metrics import auc,roc_curve,roc_auc_score\n",
    "\n",
    "lgr = LogisticRegression()\n",
    "rf = RandomForestClassifier(n_jobs=2, random_state=25,n_estimators=100)\n",
    "svm = SVC(kernel='linear',probability=True,degree=3, gamma=1.0) #linear\n",
    "xgb = XGBClassifier(learning_rate=0.3, max_depth=4, eta=0.2)\n",
    "\n",
    "models = [lgr,rf,svm,xgb]\n",
    "\n",
    "models_name = ['LR','RF','SVM','XGB']\n",
    "color = ['r','g','b','y']\n",
    "n_fold = 5\n",
    "\n",
    "AUC_train = np.zeros((4, n_fold), dtype=np.float)\n",
    "Accuracy_train = np.zeros((4, n_fold), dtype=np.float)\n",
    "Specificity_train = np.zeros((4, n_fold), dtype=np.float)\n",
    "Sensitivity_train = np.zeros((4, n_fold), dtype=np.float)\n",
    "F1_score_train =  np.zeros((4, n_fold), dtype=np.float)\n",
    "AUC_test =  np.zeros((4, n_fold), dtype=np.float)\n",
    "Accuracy_test = np.zeros((4, n_fold), dtype=np.float)\n",
    "Specificity_test =  np.zeros((4, n_fold), dtype=np.float)\n",
    "Sensitivity_test =  np.zeros((4, n_fold), dtype=np.float)\n",
    "F1_score_test =  np.zeros((4, n_fold), dtype=np.float)\n",
    "\n",
    "kFold = KFold(n_splits=n_fold, random_state=42, shuffle=True)\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "for index,train_model in enumerate(models):  \n",
    "    i = 0\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    CIs = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    for train_index, test_index in kFold.split(X_Flair,Y_data):\n",
    "    #print(\"Train Index: \", train_index, \"\\n\")\n",
    "    #print(\"Test Index: \", test_index)\n",
    "        print(\"begin to select features\")\n",
    "        X_Flair_train, X_Flair_test, y_train, y_test = X_Flair_data[train_index], X_Flair_data[test_index], Y_data[train_index], Y_data[test_index]\n",
    "        X_T2_train, X_T2_test, y_train, y_test = X_T2_data[train_index], X_T2_data[test_index], Y_data[train_index], Y_data[test_index]\n",
    "        X_T1_train, X_T1_test, y_train, y_test = X_T1_data[train_index], X_T1_data[test_index], Y_data[train_index], Y_data[test_index]\n",
    "        X_T1ce_train, X_T1ce_test, y_train, y_test = X_T1ce_data[train_index], X_T1ce_data[test_index], Y_data[train_index], Y_data[test_index]\n",
    "\n",
    "        X_Flair_train = pd.DataFrame(X_Flair_train, columns=X_Flair.columns)\n",
    "        X_T2_train = pd.DataFrame(X_T2_train, columns=X_T2.columns)\n",
    "        X_T1_train = pd.DataFrame(X_T1_train, columns=X_T1.columns)\n",
    "        X_T1ce_train = pd.DataFrame(X_T1ce_train, columns=X_T1ce.columns)\n",
    "\n",
    "        X_Flair_test = pd.DataFrame(X_Flair_test, columns=X_Flair.columns)\n",
    "        X_T2_test = pd.DataFrame(X_T2_test, columns=X_T2.columns)\n",
    "        X_T1_test = pd.DataFrame(X_T1_test, columns=X_T1.columns)\n",
    "        X_T1ce_test = pd.DataFrame(X_T1ce_test, columns=X_T1ce.columns)\n",
    "\n",
    "        y_train = pd.DataFrame(y_train, columns=Y_T1.columns)\n",
    "        y_test = pd.DataFrame(y_test, columns=Y_T1.columns)\n",
    "\n",
    "        ####特征选择方法\n",
    "        feature_flair = feacture_selection(X_Flair_train,y_train)\n",
    "        feature_t2 = feacture_selection(X_T2_train,y_train)\n",
    "        feature_t1 = feacture_selection(X_T1_train,y_train)\n",
    "        feature_t1ce = feacture_selection(X_T1ce_train,y_train)\n",
    "\n",
    "        print(\"the number of feature_flair is \",len(feature_flair),\"\\n\",feature_flair)\n",
    "        print(\"the number of feature_t2 is \",len(feature_t2),\"\\n\",feature_t2)\n",
    "        print(\"the number of feature_t1 is \",len(feature_t1),\"\\n\",feature_t1)\n",
    "        print(\"the number of feature_t1ce is \",len(feature_t1ce),\"\\n\",feature_t1ce)\n",
    "        mutil_train_data = pd.concat([X_Flair_train[feature_flair],X_T2_train[feature_t2],X_T1_train[feature_t1],X_T1ce_train[feature_t1ce]],axis=1)\n",
    "        \n",
    "        final_feature = mRMR_feature_select(mutil_train_data,y_train,20)\n",
    "       \n",
    "        mutil_train_data = pd.concat([X_Flair_train[feature_flair],X_T2_train[feature_t2],X_T1_train[feature_t1],X_T1ce_train[feature_t1ce]],axis=1)\n",
    "        mutil_test_data = pd.concat([X_Flair_test[feature_flair],X_T2_test[feature_t2],X_T1_test[feature_t1],X_T1ce_test[feature_t1ce]],axis=1)\n",
    "       \n",
    "        x_smote_train,y_smote_train = smote_augment_data(mutil_train_data[final_feature],y_train)\n",
    "        \n",
    "        model =train_model.fit(x_smote_train,y_smote_train )\n",
    "        ###训练集预测\n",
    "        y_train_pred_class = model.predict(mutil_train_data[final_feature])\n",
    "                \n",
    "        y_train_pred = model.predict_proba(mutil_train_data[final_feature])[:,1]\n",
    "              \n",
    "        auc_train,accuracy,specificity,sensitivity,F1_score = evalution_metirc(y_train,y_train_pred,y_train_pred_class,labels=[0,1],target_names=[\"LGG\",\"HGG\"])\n",
    "        AUC_train[index,i] = auc_train\n",
    "        Accuracy_train[index,i] = accuracy\n",
    "        Specificity_train[index,i] = specificity\n",
    "        Sensitivity_train[index,i] = sensitivity\n",
    "        F1_score_train[index,i] = F1_score\n",
    "       \n",
    "        \n",
    "        ### 测试集\n",
    "        y_test_pred_class = model.predict(mutil_test_data[final_feature])             \n",
    "        y_test_pred = model.predict_proba(mutil_test_data[final_feature])[:,1]\n",
    "    \n",
    "        auc_1,accuracy_1,specificity_1,sensitivity_1,F1_score_1 = evalution_metirc(y_test,y_test_pred,y_test_pred_class,labels=[0,1],target_names=[\"LGG\",\"HGG\"])\n",
    "        AUC_test[index,i] = auc_1\n",
    "        Accuracy_test[index,i] = accuracy_1\n",
    "        Specificity_test[index,i] = specificity_1\n",
    "        Sensitivity_test[index,i] = sensitivity_1   \n",
    "        F1_score_test[index,i] = F1_score_1  \n",
    "    \n",
    "        \n",
    "        ###绘制ROC 曲线\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_test_pred)\n",
    "        interp_tpr = interp(mean_fpr, fpr, tpr)\n",
    "        interp_tpr[0] = 0.0\n",
    "        roc_auc = auc(fpr,tpr)\n",
    "        tprs.append(interp_tpr)\n",
    "        aucs.append(roc_auc)\n",
    "        i = i+1\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "    ax.plot(mean_fpr, mean_tpr, color=color[index],\n",
    "                label=r'Mean ROC of %s (AUC = %0.2f $\\pm$ %0.2f)' % (models_name[index], mean_auc, std_auc),\n",
    "                lw=2, alpha=.9)\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    # ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "    #                 label=r'$\\pm$ 1 std. dev.')\n",
    "    font1 = {'family': 'Times New Roman',\n",
    "             'weight': 'normal',\n",
    "             'size': 12\n",
    "             }\n",
    "    ax.set_xlabel('False Positive Rate', fontdict=font1)\n",
    "    ax.set_ylabel('True Positive Rate', fontdict=font1)\n",
    "    plt.xticks(fontsize=12, rotation=0)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.title(label='Receiver operating characteristic on FLAIR',fontdict=font1)\n",
    "    # ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n",
    "    #        title=\"Receiver operating characteristic on FLAIR\")\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color=(0.6, 0.6, 0.6),\n",
    "            label='Chance', alpha=.8)\n",
    "    # score, permutation_scores, pvalue = permutation_test_score(\n",
    "    #     model, X, y, scoring=\"roc_auc\", cv=cv, n_permutations=100, n_jobs=1)\n",
    "    # print('score:', score, 'permutation_scores:', permutation_scores, 'pvalue:', pvalue)\n",
    "plt.savefig('ROC_Nulti_ED.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for i in range(0,4,1):\n",
    "    \n",
    "    print(\"++++++++++++++++++++++the {}th model+++++++++++++++\".format(i))\n",
    "    print(\"###############训练集####################\")\n",
    "    print(\"AUC:\",AUC_train[i,:],\"average:\",np.mean(AUC_train[i,:]),\"std:\",np.std(AUC_train[i,:]))\n",
    "    AUCLIST = AUC_train[i,:]\n",
    "    \n",
    "    print(\"Accuracy:\",Accuracy_train[i,:],\"average:\",np.mean(Accuracy_train[i,:]),\"std:\",np.std(Accuracy_train[i,:]))\n",
    "    ACCURACYLIST = Accuracy_train[i,:]\n",
    "    print(\"Specificity:\",Specificity_train[i,:],\"average:\",np.mean(Specificity_train[i,:]),\"std:\",np.std(Specificity_train[i,:]))\n",
    "    SPECIFICITYLIST = Specificity_train[i,:]\n",
    "    print(\"Sensitivity:\",Sensitivity_train[i,:],\"average:\",np.mean(Sensitivity_train[i,:]),\"std:\",np.std(Sensitivity_train[i,:]))\n",
    "    SENSITIVITYLIST = Sensitivity_train[i,:]\n",
    "    \n",
    "    print(\"###############测试集####################\")\n",
    "    \n",
    "    print(\"AUC:\",AUC_test[i,:],\"average:\",np.mean(AUC_test[i,:]),\"std:\",np.std(AUC_test[i,:]))\n",
    "    AUCLIST_1 = AUC_test[i,:]\n",
    "    print(\"Accuracy:\",Accuracy_test[i,:],\"average:\",np.mean(Accuracy_test[i,:]),\"std:\",np.std(Accuracy_test[i,:]))\n",
    "    ACCURACYLIST_1 = Accuracy_test[i,:]\n",
    "    print(\"Specificity:\",Specificity_test[i,:],\"average:\",np.mean(Specificity_test[i,:]),\"std:\",np.std(Specificity_test[i,:]))\n",
    "    SPECIFICITYLIST_1 = Specificity_test[i,:]\n",
    "    print(\"Sensitivity:\",Sensitivity_test[i,:],\"average:\",np.mean(Sensitivity_test[i,:]),\"std:\",np.std(Sensitivity_test[i,:])) \n",
    "    SENSITIVITYLIST_1 = Sensitivity_test[i,:]\n",
    "    print(\"F1_score:\",F1_score_test[i,:],\"average:\",np.mean(F1_score_test[i,:]),\"std:\",np.std(F1_score_test[i,:])) \n",
    "    F1_SCORE_LIST = F1_score_test[i,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
